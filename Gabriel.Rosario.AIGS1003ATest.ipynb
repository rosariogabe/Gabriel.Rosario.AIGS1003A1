{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5385122a",
   "metadata": {},
   "source": [
    "# File \"util.py\"\n",
    "Data structures useful for implementing SearchAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ca641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "import heapq, random\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " Data structures useful for implementing SearchAgents\n",
    "\"\"\"\n",
    "\n",
    "class Stack:\n",
    "  \"A container with a last-in-first-out (LIFO) queuing policy.\"\n",
    "  def __init__(self):\n",
    "    self.list = []\n",
    "    \n",
    "  def push(self,item):\n",
    "    \"Push 'item' onto the stack\"\n",
    "    self.list.append(item)\n",
    "\n",
    "  def pop(self):\n",
    "    \"Pop the most recently pushed item from the stack\"\n",
    "    return self.list.pop()\n",
    "\n",
    "  def isEmpty(self):\n",
    "    \"Returns true if the stack is empty\"\n",
    "    return len(self.list) == 0\n",
    "\n",
    "class Queue:\n",
    "  \"A container with a first-in-first-out (FIFO) queuing policy.\"\n",
    "  def __init__(self):\n",
    "    self.list = []\n",
    "  \n",
    "  def push(self,item):\n",
    "    \"Enqueue the 'item' into the queue\"\n",
    "    self.list.insert(0,item)\n",
    "\n",
    "  def pop(self):\n",
    "    \"\"\"\n",
    "      Dequeue the earliest enqueued item still in the queue. This\n",
    "      operation removes the item from the queue.\n",
    "    \"\"\"\n",
    "    return self.list.pop()\n",
    "\n",
    "  def isEmpty(self):\n",
    "    \"Returns true if the queue is empty\"\n",
    "    return len(self.list) == 0\n",
    "  \n",
    "class PriorityQueue:\n",
    "  \"\"\"\n",
    "    Implements a priority queue data structure. Each inserted item\n",
    "    has a priority associated with it and the client is usually interested\n",
    "    in quick retrieval of the lowest-priority item in the queue. This\n",
    "    data structure allows O(1) access to the lowest-priority item.\n",
    "    \n",
    "    Note that this PriorityQueue does not allow you to change the priority\n",
    "    of an item.  However, you may insert the same item multiple times with\n",
    "    different priorities.\n",
    "  \"\"\"  \n",
    "  def  __init__(self):  \n",
    "    self.heap = []\n",
    "    \n",
    "  def push(self, item, priority):\n",
    "      pair = (priority,item)\n",
    "      heapq.heappush(self.heap,pair)\n",
    "\n",
    "  def pop(self):\n",
    "      (priority,item) = heapq.heappop(self.heap)\n",
    "      return item\n",
    "  \n",
    "  def isEmpty(self):\n",
    "    return len(self.heap) == 0\n",
    "\n",
    "class PriorityQueueWithFunction(PriorityQueue):\n",
    "  \"\"\"\n",
    "  Implements a priority queue with the same push/pop signature of the\n",
    "  Queue and the Stack classes. This is designed for drop-in replacement for\n",
    "  those two classes. The caller has to provide a priority function, which\n",
    "  extracts each item's priority.\n",
    "  \"\"\"  \n",
    "  def  __init__(self, priorityFunction):\n",
    "    \"priorityFunction (item) -> priority\"\n",
    "    self.priorityFunction = priorityFunction      # store the priority function\n",
    "    PriorityQueue.__init__(self)        # super-class initializer\n",
    "    \n",
    "  def push(self, item):\n",
    "    \"Adds an item to the queue with priority from the priority function\"\n",
    "    PriorityQueue.push(self, item, self.priorityFunction(item))\n",
    "\n",
    "    \n",
    "def manhattanDistance( xy1, xy2 ):\n",
    "  \"Returns the Manhattan distance between points xy1 and xy2\"\n",
    "  return abs( xy1[0] - xy2[0] ) + abs( xy1[1] - xy2[1] )\n",
    "\n",
    "\"\"\"\n",
    "  Data structures and functions useful for various course projects\n",
    "  \n",
    "  The search project should not need anything below this line.\n",
    "\"\"\"\n",
    "\n",
    "class Counter(dict):\n",
    "  \"\"\"\n",
    "  A counter keeps track of counts for a set of keys.\n",
    "  \n",
    "  The counter class is an extension of the standard python\n",
    "  dictionary type.  It is specialized to have number values  \n",
    "  (integers or floats), and includes a handful of additional\n",
    "  functions to ease the task of counting data.  In particular, \n",
    "  all keys are defaulted to have value 0.  Using a dictionary:\n",
    "  \n",
    "  a = {}\n",
    "  print a['test']\n",
    "  \n",
    "  would give an error, while the Counter class analogue:\n",
    "    \n",
    "  >>> a = Counter()\n",
    "  >>> print a['test']\n",
    "  0\n",
    "\n",
    "  returns the default 0 value. Note that to reference a key \n",
    "  that you know is contained in the counter, \n",
    "  you can still use the dictionary syntax:\n",
    "    \n",
    "  >>> a = Counter()\n",
    "  >>> a['test'] = 2\n",
    "  >>> print a['test']\n",
    "  2\n",
    "  \n",
    "  This is very useful for counting things without initializing their counts,\n",
    "  see for example:\n",
    "  \n",
    "  >>> a['blah'] += 1\n",
    "  >>> print a['blah']\n",
    "  1\n",
    "  \n",
    "  The counter also includes additional functionality useful in implementing\n",
    "  the classifiers for this assignment.  Two counters can be added,\n",
    "  subtracted or multiplied together.  See below for details.  They can\n",
    "  also be normalized and their total count and arg max can be extracted.\n",
    "  \"\"\"\n",
    "  def __getitem__(self, idx):\n",
    "    self.setdefault(idx, 0)\n",
    "    return dict.__getitem__(self, idx)\n",
    "\n",
    "  def incrementAll(self, keys, count):\n",
    "    \"\"\"\n",
    "    Increments all elements of keys by the same count.\n",
    "    \n",
    "    >>> a = Counter()\n",
    "    >>> a.incrementAll(['one','two', 'three'], 1)\n",
    "    >>> a['one']\n",
    "    1\n",
    "    >>> a['two']\n",
    "    1\n",
    "    \"\"\"\n",
    "    for key in keys:\n",
    "      self[key] += count\n",
    "  \n",
    "  def argMax(self):\n",
    "    \"\"\"\n",
    "    Returns the key with the highest value.\n",
    "    \"\"\"\n",
    "    if len(list(self.keys())) == 0: return None\n",
    "    all = list(self.items())\n",
    "    values = [x[1] for x in all]\n",
    "    maxIndex = values.index(max(values))\n",
    "    return all[maxIndex][0]\n",
    "  \n",
    "  def sortedKeys(self):\n",
    "    \"\"\"\n",
    "    Returns a list of keys sorted by their values.  Keys\n",
    "    with the highest values will appear first.\n",
    "    \n",
    "    >>> a = Counter()\n",
    "    >>> a['first'] = -2\n",
    "    >>> a['second'] = 4\n",
    "    >>> a['third'] = 1\n",
    "    >>> a.sortedKeys()\n",
    "    ['second', 'third', 'first']\n",
    "    \"\"\"\n",
    "    sortedItems = list(self.items())\n",
    "    compare = lambda x, y:  sign(y[1] - x[1])\n",
    "    sortedItems.sort(cmp=compare)\n",
    "    return [x[0] for x in sortedItems]\n",
    "  \n",
    "  def totalCount(self):\n",
    "    \"\"\"\n",
    "    Returns the sum of counts for all keys.\n",
    "    \"\"\"\n",
    "    return sum(self.values())\n",
    "  \n",
    "  def normalize(self):\n",
    "    \"\"\"\n",
    "    Edits the counter such that the total count of all\n",
    "    keys sums to 1.  The ratio of counts for all keys\n",
    "    will remain the same. Note that normalizing an empty \n",
    "    Counter will result in an error.\n",
    "    \"\"\"\n",
    "    total = float(self.totalCount())\n",
    "    if total == 0: return\n",
    "    for key in list(self.keys()):\n",
    "      self[key] = self[key] / total\n",
    "      \n",
    "  def divideAll(self, divisor):\n",
    "    \"\"\"\n",
    "    Divides all counts by divisor\n",
    "    \"\"\"\n",
    "    divisor = float(divisor)\n",
    "    for key in self:\n",
    "      self[key] /= divisor\n",
    "\n",
    "  def copy(self):\n",
    "    \"\"\"\n",
    "    Returns a copy of the counter\n",
    "    \"\"\"\n",
    "    return Counter(dict.copy(self))\n",
    "  \n",
    "  def __mul__(self, y ):\n",
    "    \"\"\"\n",
    "    Multiplying two counters gives the dot product of their vectors where\n",
    "    each unique label is a vector element.\n",
    "    \n",
    "    >>> a = Counter()\n",
    "    >>> b = Counter()\n",
    "    >>> a['first'] = -2\n",
    "    >>> a['second'] = 4\n",
    "    >>> b['first'] = 3\n",
    "    >>> b['second'] = 5\n",
    "    >>> a['third'] = 1.5\n",
    "    >>> a['fourth'] = 2.5\n",
    "    >>> a * b\n",
    "    14\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    x = self\n",
    "    if len(x) > len(y):\n",
    "      x,y = y,x\n",
    "    for key in x:\n",
    "      if key not in y:\n",
    "        continue\n",
    "      sum += x[key] * y[key]      \n",
    "    return sum\n",
    "      \n",
    "  def __radd__(self, y):\n",
    "    \"\"\"\n",
    "    Adding another counter to a counter increments the current counter\n",
    "    by the values stored in the second counter.\n",
    "    \n",
    "    >>> a = Counter()\n",
    "    >>> b = Counter()\n",
    "    >>> a['first'] = -2\n",
    "    >>> a['second'] = 4\n",
    "    >>> b['first'] = 3\n",
    "    >>> b['third'] = 1\n",
    "    >>> a += b\n",
    "    >>> a['first']\n",
    "    1\n",
    "    \"\"\" \n",
    "    for key, value in list(y.items()):\n",
    "      self[key] += value   \n",
    "      \n",
    "  def __add__( self, y ):\n",
    "    \"\"\"\n",
    "    Adding two counters gives a counter with the union of all keys and\n",
    "    counts of the second added to counts of the first.\n",
    "    \n",
    "    >>> a = Counter()\n",
    "    >>> b = Counter()\n",
    "    >>> a['first'] = -2\n",
    "    >>> a['second'] = 4\n",
    "    >>> b['first'] = 3\n",
    "    >>> b['third'] = 1\n",
    "    >>> (a + b)['first']\n",
    "    1\n",
    "    \"\"\"\n",
    "    addend = Counter()\n",
    "    for key in self:\n",
    "      if key in y:\n",
    "        addend[key] = self[key] + y[key]\n",
    "      else:\n",
    "        addend[key] = self[key]\n",
    "    for key in y:\n",
    "      if key in self:\n",
    "        continue\n",
    "      addend[key] = y[key]\n",
    "    return addend\n",
    "    \n",
    "  def __sub__( self, y ):\n",
    "    \"\"\"\n",
    "    Subtracting a counter from another gives a counter with the union of all keys and\n",
    "    counts of the second subtracted from counts of the first.\n",
    "    \n",
    "    >>> a = Counter()\n",
    "    >>> b = Counter()\n",
    "    >>> a['first'] = -2\n",
    "    >>> a['second'] = 4\n",
    "    >>> b['first'] = 3\n",
    "    >>> b['third'] = 1\n",
    "    >>> (a - b)['first']\n",
    "    -5\n",
    "    \"\"\"      \n",
    "    addend = Counter()\n",
    "    for key in self:\n",
    "      if key in y:\n",
    "        addend[key] = self[key] - y[key]\n",
    "      else:\n",
    "        addend[key] = self[key]\n",
    "    for key in y:\n",
    "      if key in self:\n",
    "        continue\n",
    "      addend[key] = -1 * y[key]\n",
    "    return addend\n",
    "    \n",
    "def raiseNotDefined():\n",
    "  print(\"Method not implemented: %s\" % inspect.stack()[1][3])    \n",
    "  sys.exit(1)\n",
    "\n",
    "def normalize(vectorOrCounter):\n",
    "  \"\"\"\n",
    "  normalize a vector or counter by dividing each value by the sum of all values\n",
    "  \"\"\"\n",
    "  normalizedCounter = Counter()\n",
    "  if type(vectorOrCounter) == type(normalizedCounter):\n",
    "    counter = vectorOrCounter\n",
    "    total = float(counter.totalCount())\n",
    "    if total == 0: return counter\n",
    "    for key in list(counter.keys()):\n",
    "      value = counter[key]\n",
    "      normalizedCounter[key] = value / total\n",
    "    return normalizedCounter\n",
    "  else:\n",
    "    vector = vectorOrCounter\n",
    "    s = float(sum(vector))\n",
    "    if s == 0: return vector\n",
    "    return [el / s for el in vector]\n",
    "                \n",
    "def nSample(distribution, values, n):\n",
    "  if sum(distribution) != 1:\n",
    "    distribution = normalize(distribution)\n",
    "  rand = [random.random() for i in range(n)]\n",
    "  rand.sort()\n",
    "  samples = []\n",
    "  samplePos, distPos, cdf = 0,0, distribution[0]\n",
    "  while samplePos < n:\n",
    "    if rand[samplePos] < cdf:\n",
    "      samplePos += 1\n",
    "      samples.append(values[distPos])\n",
    "    else:\n",
    "      distPos += 1\n",
    "      cdf += distribution[distPos]\n",
    "  return samples\n",
    "    \n",
    "def sample(distribution, values = None):\n",
    "  if type(distribution) == Counter: \n",
    "    items = list(distribution.items())\n",
    "    distribution = [i[1] for i in items] \n",
    "    values = [i[0] for i in items] \n",
    "  if sum(distribution) != 1:\n",
    "    distribution = normalize(distribution)\n",
    "  choice = random.random()\n",
    "  i, total= 0, distribution[0]\n",
    "  while choice > total:\n",
    "    i += 1\n",
    "    total += distribution[i]\n",
    "  return values[i]\n",
    "\n",
    "def sampleFromCounter(ctr):\n",
    "  items = list(ctr.items())\n",
    "  return sample([v for k,v in items], [k for k,v in items])\n",
    "\n",
    "def getProbability(value, distribution, values):\n",
    "  \"\"\"\n",
    "    Gives the probability of a value under a discrete distribution\n",
    "    defined by (distributions, values).\n",
    "  \"\"\"\n",
    "  total = 0.0\n",
    "  for prob, val in zip(distribution, values):\n",
    "    if val == value:\n",
    "      total += prob\n",
    "  return total\n",
    "\n",
    "def flipCoin( p ):\n",
    "  r = random.random()\n",
    "  return r < p \n",
    "\n",
    "def chooseFromDistribution( distribution ):\n",
    "  \"Takes either a counter or a list of (prob, key) pairs and samples\"\n",
    "  if type(distribution) == dict or type(distribution) == Counter:\n",
    "    return sample(distribution)\n",
    "  r = random.random()\n",
    "  base = 0.0\n",
    "  for prob, element in distribution:\n",
    "    base += prob\n",
    "    if r <= base: return element\n",
    "    \n",
    "def nearestPoint( pos ):\n",
    "  \"\"\"\n",
    "  Finds the nearest grid point to a position (discretizes).\n",
    "  \"\"\"\n",
    "  ( current_row, current_col ) = pos\n",
    "\n",
    "  grid_row = int( current_row + 0.5 ) \n",
    "  grid_col = int( current_col + 0.5 ) \n",
    "  return ( grid_row, grid_col )     \n",
    "\n",
    "def sign( x ):\n",
    "  \"\"\"\n",
    "  Returns 1 or -1 depending on the sign of x\n",
    "  \"\"\"\n",
    "  if( x >= 0 ):\n",
    "    return 1\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "def arrayInvert(array):\n",
    "  \"\"\"\n",
    "  Inverts a matrix stored as a list of lists.\n",
    "  \"\"\"\n",
    "  result = [[] for i in array]\n",
    "  for outer in array:\n",
    "    for inner in range(len(outer)):\n",
    "      result[inner].append(outer[inner])\n",
    "  return result\n",
    "\n",
    "def matrixAsList( matrix, value = True ):\n",
    "  \"\"\"\n",
    "  Turns a matrix into a list of coordinates matching the specified value\n",
    "  \"\"\"\n",
    "  rows, cols = len( matrix ), len( matrix[0] )\n",
    "  cells = []\n",
    "  for row in range( rows ):\n",
    "    for col in range( cols ):\n",
    "      if matrix[row][col] == value:\n",
    "        cells.append( ( row, col ) )\n",
    "  return cells\n",
    "\n",
    "def lookup(name, namespace):\n",
    "  \"\"\"\n",
    "  Get a method or class from any imported module from its name.\n",
    "  Usage: lookup(functionName, globals())\n",
    "  \"\"\"\n",
    "  dots = name.count('.')\n",
    "  if dots > 0:\n",
    "    moduleName, objName = '.'.join(name.split('.')[:-1]), name.split('.')[-1]\n",
    "    module = __import__(moduleName)\n",
    "    return getattr(module, objName)\n",
    "  else:\n",
    "    modules = [obj for obj in list(namespace.values()) if str(type(obj)) == \"<type 'module'>\"]\n",
    "    options = [getattr(module, name) for module in modules if name in dir(module)]\n",
    "    options += [obj[1] for obj in list(namespace.items()) if obj[0] == name ]\n",
    "    if len(options) == 1: return options[0]\n",
    "    if len(options) > 1: raise Exception('Name conflict for %s')\n",
    "    raise Exception('%s not found as a method or class' % name)\n",
    "\n",
    "def pause():\n",
    "  \"\"\"\n",
    "  Pauses the output stream awaiting user feedback.\n",
    "  \"\"\"\n",
    "  print(\"<Press enter/return to continue>\")\n",
    "  input()\n",
    "  \n",
    "  \n",
    "## code to handle timeouts\n",
    "import signal\n",
    "class TimeoutFunctionException(Exception):\n",
    "    \"\"\"Exception to raise on a timeout\"\"\"\n",
    "    pass\n",
    "\n",
    "class TimeoutFunction:\n",
    "\n",
    "    def __init__(self, function, timeout):\n",
    "        \"timeout must be at least 1 second. WHY??\"\n",
    "        self.timeout = timeout\n",
    "        self.function = function\n",
    "\n",
    "    def handle_timeout(self, signum, frame):\n",
    "        raise TimeoutFunctionException()\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if not 'SIGALRM' in dir(signal):\n",
    "            return self.function(*args)\n",
    "        old = signal.signal(signal.SIGALRM, self.handle_timeout)\n",
    "        signal.alarm(self.timeout)\n",
    "        try:\n",
    "            result = self.function(*args)\n",
    "        finally:\n",
    "            signal.signal(signal.SIGALRM, old)\n",
    "        signal.alarm(0)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b3e50ab",
   "metadata": {},
   "source": [
    "# File \"dataClassifier.py\"\n",
    "# This file contains feature extraction methods and harness \n",
    "# code for data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e96a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains feature extraction methods and harness \n",
    "# code for data classification\n",
    "\n",
    "import mostFrequent\n",
    "import naiveBayes\n",
    "import samples\n",
    "import sys\n",
    "import util\n",
    "\n",
    "#Test\n",
    "TEST_SET_SIZE = 300\n",
    "#Image Size\n",
    "DIGIT_DATUM_WIDTH=28\n",
    "DIGIT_DATUM_HEIGHT=28\n",
    "FACE_DATUM_WIDTH=60\n",
    "FACE_DATUM_HEIGHT=70\n",
    "\n",
    "\n",
    "def basicFeatureExtractorDigit(datum):\n",
    "  \"\"\"\n",
    "  Returns a set of pixel features indicating whether\n",
    "  each pixel in the provided datum is white (0) or gray/black (1)\n",
    "  \"\"\"\n",
    "  a = datum.getPixels()\n",
    "\n",
    "  features = util.Counter()\n",
    "  for x in range(DIGIT_DATUM_WIDTH):\n",
    "    for y in range(DIGIT_DATUM_HEIGHT):\n",
    "      if datum.getPixel(x, y) > 0:\n",
    "        features[(x,y)] = 1\n",
    "      else:\n",
    "        features[(x,y)] = 0\n",
    "  return features\n",
    "\n",
    "\n",
    "def analysis(classifier, guesses, testLabels, testData, rawTestData, printImage):\n",
    "  \"\"\"\n",
    "  This function is called after learning.\n",
    "  Include any code that you want here to help you analyze your results.\n",
    "  \n",
    "  Use the printImage(<list of pixels>) function to visualize features.\n",
    "  \n",
    "  An example of use has been given to you.\n",
    "  \n",
    "  - classifier is the trained classifier\n",
    "  - guesses is the list of labels predicted by your classifier on the test set\n",
    "  - testLabels is the list of true labels\n",
    "  - testData is the list of training datapoints (as util.Counter of features)\n",
    "  - rawTestData is the list of training datapoints (as samples.Datum)\n",
    "  - printImage is a method to visualize the features \n",
    "  (see its use in the odds ratio part in runClassifier method)\n",
    "  \n",
    "  This code won't be evaluated. It is for your own optional use\n",
    "  (and you can modify the signature if you want).\n",
    "  \"\"\"\n",
    "  \n",
    "  # Put any code here...\n",
    "  # Example of use:\n",
    "  for i in range(len(guesses)):\n",
    "      prediction = guesses[i]\n",
    "      truth = testLabels[i]\n",
    "      if (prediction != truth):\n",
    "          print(\"===================================\")\n",
    "          print(\"Mistake on example %d\" % i) \n",
    "          print(\"Predicted %d; truth is %d\" % (prediction, truth))\n",
    "          print(\"Image: \")\n",
    "          print(rawTestData[i])\n",
    "          break\n",
    "\n",
    "\n",
    "## =====================\n",
    "## You don't have to modify any code below.\n",
    "## =====================\n",
    "\n",
    "\n",
    "class ImagePrinter:\n",
    "    def __init__(self, width, height):\n",
    "      self.width = width\n",
    "      self.height = height\n",
    "\n",
    "def default(str):\n",
    "  return str + ' [Default: %default]'\n",
    "\n",
    "def readCommand( argv ):\n",
    "  \"Processes the command used to run from the command line.\"\n",
    "  from optparse import OptionParser  \n",
    "  parser = OptionParser(USAGE_STRING)\n",
    "  \n",
    "  parser.add_option('-c', '--classifier', help=default('The type of classifier'), choices=['mostFrequent', 'nb', 'naiveBayes', 'perceptron', 'mira', 'minicontest'], default='mostFrequent')\n",
    "  parser.add_option('-d', '--data', help=default('Dataset to use'), choices=['digits', 'faces'], default='digits')\n",
    "  parser.add_option('-t', '--training', help=default('The size of the training set'), default=100, type=\"int\")\n",
    "  parser.add_option('-a', '--autotune', help=default(\"Whether to automatically tune hyperparameters\"), default=False, action=\"store_true\")\n",
    "  parser.add_option('-i', '--iterations', help=default(\"Maximum iterations to run training\"), default=3, type=\"int\")\n",
    "\n",
    "  options, otherjunk = parser.parse_args(argv)\n",
    "  if len(otherjunk) != 0: raise Exception('Command line input not understood: ' + str(otherjunk))\n",
    "  args = {}\n",
    "  \n",
    "  # Set up variables according to the command line input.\n",
    "  print(\"Doing classification\")\n",
    "  print(\"--------------------\")\n",
    "  print(\"data:\\t\\t\" + options.data)\n",
    "  print(\"classifier:\\t\\t\" + options.classifier)\n",
    "  print(\"training set size:\\t\" + str(options.training))\n",
    "  if(options.data==\"digits\"):\n",
    "    printImage = ImagePrinter(DIGIT_DATUM_WIDTH, DIGIT_DATUM_HEIGHT)\n",
    "    featureFunction = basicFeatureExtractorDigit    \n",
    "  else:\n",
    "    print(\"Unknown dataset\", options.data)\n",
    "    print(USAGE_STRING)\n",
    "    sys.exit(2)\n",
    "    \n",
    "  if(options.data==\"digits\"):\n",
    "    legalLabels = list(range(10))\n",
    "  else:\n",
    "    legalLabels = list(range(2))\n",
    "    \n",
    "  if options.training <= 0:\n",
    "    print(\"Training set size should be a positive integer (you provided: %d)\" % options.training)\n",
    "    print(USAGE_STRING)\n",
    "    sys.exit(2)\n",
    "\n",
    "  if(options.classifier == \"mostFrequent\"):\n",
    "    classifier = mostFrequent.MostFrequentClassifier(legalLabels)\n",
    "  elif(options.classifier == \"naiveBayes\" or options.classifier == \"nb\"):\n",
    "    classifier = naiveBayes.NaiveBayesClassifier(legalLabels)\n",
    "    if (options.autotune):\n",
    "        print(\"using automatic tuning for naivebayes\")\n",
    "        classifier.automaticTuning = True\n",
    "  else:\n",
    "    print(\"Unknown classifier:\", options.classifier)\n",
    "    print(USAGE_STRING)\n",
    "    \n",
    "    sys.exit(2)\n",
    "\n",
    "  args['classifier'] = classifier\n",
    "  args['featureFunction'] = featureFunction\n",
    "  args['printImage'] = printImage\n",
    "  \n",
    "  return args, options\n",
    "\n",
    "USAGE_STRING = \"\"\"\n",
    "  USAGE:      python dataClassifier.py <options>\n",
    "  EXAMPLES:   (1) python dataClassifier.py\n",
    "                  - trains the default mostFrequent classifier on the digit dataset\n",
    "                  using the default 100 training examples and\n",
    "                  then test the classifier on test data\n",
    "                 \"\"\"\n",
    "\n",
    "# Main harness code\n",
    "\n",
    "def runClassifier(args, options):\n",
    "\n",
    "  featureFunction = args['featureFunction']\n",
    "  classifier = args['classifier']\n",
    "  printImage = args['printImage']\n",
    "      \n",
    "  # Load data  \n",
    "  numTraining = options.training\n",
    "\n",
    "  rawTrainingData = samples.loadDataFile(\"trainingimages\", numTraining,DIGIT_DATUM_WIDTH,DIGIT_DATUM_HEIGHT)\n",
    "  trainingLabels = samples.loadLabelsFile(\"traininglabels\", numTraining)\n",
    "  rawValidationData = samples.loadDataFile(\"validationimages\", TEST_SET_SIZE,DIGIT_DATUM_WIDTH,DIGIT_DATUM_HEIGHT)\n",
    "  validationLabels = samples.loadLabelsFile(\"validationlabels\", TEST_SET_SIZE)\n",
    "  rawTestData = samples.loadDataFile(\"testimages\", TEST_SET_SIZE,DIGIT_DATUM_WIDTH,DIGIT_DATUM_HEIGHT)\n",
    "  testLabels = samples.loadLabelsFile(\"testlabels\", TEST_SET_SIZE)\n",
    "    \n",
    "  \n",
    "  # Extract features\n",
    "  print(\"Extracting features...\")\n",
    "  trainingData = list(map(featureFunction, rawTrainingData))\n",
    "  validationData = list(map(featureFunction, rawValidationData))\n",
    "  testData = list(map(featureFunction, rawTestData))\n",
    "  \n",
    "  # Conduct training and testing\n",
    "  print(\"Training...\")\n",
    "  classifier.train(trainingData, trainingLabels, validationData, validationLabels)\n",
    "  print(\"Validating...\")\n",
    "  guesses = classifier.classify(validationData)\n",
    "  correct = [guesses[i] == validationLabels[i] for i in range(len(validationLabels))].count(True)\n",
    "  print(str(correct), (\"correct out of \" + str(len(validationLabels)) + \" (%.1f%%).\") % (100.0 * correct / len(validationLabels)))\n",
    "  print(\"Testing...\")\n",
    "  guesses = classifier.classify(testData)\n",
    "  correct = [guesses[i] == testLabels[i] for i in range(len(testLabels))].count(True)\n",
    "  print(str(correct), (\"correct out of \" + str(len(testLabels)) + \" (%.1f%%).\") % (100.0 * correct / len(testLabels)))\n",
    "  analysis(classifier, guesses, testLabels, testData, rawTestData, printImage)\n",
    "\n",
    "  # Read input\n",
    "  args, options = readCommand( sys.argv[1:] ) \n",
    "  # Run classifier\n",
    "  runClassifier(args, options)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa2a9334",
   "metadata": {},
   "source": [
    "# File \"naiveBayes.py\"\n",
    "Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4745df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import classificationMethod\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "  \"\"\"\n",
    "  See the project description for the specifications of the Naive Bayes classifier.\n",
    "  \n",
    "  Note that the variable 'datum' in this code refers to a counter of features\n",
    "  (not to a raw samples.Datum).\n",
    "  \"\"\"\n",
    "  def __init__(self, legalLabels):\n",
    "    self.legalLabels = legalLabels\n",
    "    self.type = \"naivebayes\"\n",
    "    self.k = 1 # this is the smoothing parameter, ** use it in your train method **\n",
    "    self.automaticTuning = False # Look at this flag to decide whether to choose k automatically ** use this in your train method **\n",
    "    \n",
    "  def setSmoothing(self, k):\n",
    "    \"\"\"\n",
    "    This is used by the main method to change the smoothing parameter before training.\n",
    "    Do not modify this method.\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "\n",
    "  def train(self, trainingData, trainingLabels, validationData, validationLabels):\n",
    "    \"\"\"\n",
    "    Outside shell to call your method. Do not modify this method.\n",
    "    \"\"\"  \n",
    "      \n",
    "    self.features = list(trainingData[0].keys()) # this could be useful for your code later...\n",
    "    \n",
    "    if (self.automaticTuning):\n",
    "        kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "    else:\n",
    "        kgrid = [self.k]\n",
    "        \n",
    "    self.trainAndTune(trainingData, trainingLabels, validationData, validationLabels, kgrid)\n",
    "      \n",
    "  def trainAndTune(self, trainingData, trainingLabels, validationData, validationLabels, kgrid):\n",
    "    \"\"\"\n",
    "    Trains the classifier by collecting counts over the training data, and\n",
    "    stores the Laplace smoothed estimates so that they can be used to classify.\n",
    "    Evaluate each value of k in kgrid to choose the smoothing parameter \n",
    "    that gives the best accuracy on the held-out validationData.\n",
    "    \n",
    "    trainingData and validationData are lists of feature Counters.  The corresponding\n",
    "    label lists contain the correct label for each datum.\n",
    "    \n",
    "    To get the list of all possible features or labels, use self.features and \n",
    "    self.legalLabels.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    GABRIEL ROSARIO\n",
    "    Trains the classifier using Laplace smoothing and selects the best value of k\n",
    "    based on accuracy on the validation set.\n",
    "\n",
    "    Args:\n",
    "    trainingData - list of Counters - Training data.\n",
    "    trainingLabels - list of Labels for the training data.\n",
    "    validationData - list of Counters - Validation data.\n",
    "    validationLabels - list Labels for the validation data.\n",
    "    kgrid list of float  List of candidate values for the smoothing parameter k.\n",
    "    \"\"\"    \n",
    "\n",
    "    best_accuracy = -1  # Best accuracy on validation set\n",
    "    best_params = None  # Best parameters (prior, conditionalProb, k)\n",
    "\n",
    "    # Common training - get all counts from training data\n",
    "    common_prior = util.Counter()  # Probability over labels\n",
    "    common_conditional_prob = util.Counter()  # Conditional probability counters\n",
    "    common_counts = util.Counter()  # Times\n",
    "\n",
    "    for datum, label in zip(trainingData, trainingLabels):\n",
    "        common_prior[label] += 1\n",
    "        for feat, value in datum.items():\n",
    "            common_counts[(feat, label)] += 1\n",
    "            if value > 0:  # Assume a binary \n",
    "                common_conditional_prob[(feat, label)] += 1\n",
    "\n",
    "    for k in kgrid:  # Smoothing parameter tuning loop\n",
    "        prior = common_prior.copy()\n",
    "        conditional_prob = common_conditional_prob.copy()\n",
    "        counts = common_counts.copy()\n",
    "\n",
    "        # Apply Laplace smoothing\n",
    "        for label in self.legalLabels:\n",
    "            for feat in self.features:\n",
    "                conditional_prob[(feat, label)] += k\n",
    "                counts[(feat, label)] += 2 * k  # 2 because both value 0 and 1 are smoothed\n",
    "\n",
    "        # Normalize probabilities make them to sume 1\n",
    "        prior.normalize()\n",
    "        for x, count in conditional_prob.items():\n",
    "            conditional_prob[x] = count / counts[x]\n",
    "\n",
    "        self.prior = prior\n",
    "        self.conditionalProb = conditional_prob\n",
    "\n",
    "        # Evaluate performance \n",
    "        predictions = self.classify(validationData)\n",
    "        accuracy = sum(predictions[i] == validationLabels[i] for i in range(len(validationLabels))) / len(validationLabels)\n",
    "\n",
    "        print(f\"Performance validation set for k={k:.3f}: {accuracy * 100:.1f}%\")\n",
    "        if accuracy > best_accuracy:\n",
    "            best_params = (prior, conditional_prob, k)\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "        # Set the best parameters after tuning\n",
    "        self.prior, self.conditionalProb, self.k = best_params\n",
    "    #util.raiseNotDefined()\n",
    "        \n",
    "  def classify(self, testData):\n",
    "    \"\"\"\n",
    "    Classify the data based on the posterior distribution over labels.\n",
    "    \n",
    "    You shouldn't modify this method.\n",
    "    \"\"\"\n",
    "    guesses = []\n",
    "    self.posteriors = [] # Log posteriors are stored for later data analysis (autograder).\n",
    "    for datum in testData:\n",
    "      posterior = self.calculateLogJointProbabilities(datum)\n",
    "      guesses.append(posterior.argMax())\n",
    "      self.posteriors.append(posterior)\n",
    "    return guesses\n",
    "        \n",
    "  def calculateLogJointProbabilities(self, datum):\n",
    "    \"\"\"\n",
    "    Returns the log-joint distribution over legal labels and the datum.\n",
    "    Each log-probability should be stored in the log-joint counter, e.g.    \n",
    "    logJoint[3] = <Estimate of log( P(Label = 3, datum) )>\n",
    "    \"\"\"\n",
    "    #datum Counter of features    \n",
    "    #log-joint probabilities for each legal label\n",
    "    logJoint = util.Counter()\n",
    "\n",
    "    #Formula: log(P(y | x)) = log(P(y)) + Σ [x_i * log(P(x_i | y)) + (1 - x_i) * log(1 - P(x_i | y))]\n",
    "    for label in self.legalLabels:\n",
    "        logJoint[label] = math.log(self.prior[label])\n",
    "        for feat, value in datum.items():\n",
    "            conditional_prob = self.conditionalProb[(feat, label)]\n",
    "            logJoint[label] += value * math.log(conditional_prob) + (1 - value) * math.log(1 - conditional_prob)\n",
    "\n",
    "    #util.raiseNotDefined()\n",
    "    \n",
    "    return logJoint\n",
    "  \n",
    "    def findHighOddsFeatures(self, label1, label2):\n",
    "        \"\"\"\n",
    "        Returns the 100 best features for the odds ratio:\n",
    "                P(feature=1 | label1)/P(feature=1 | label2)\n",
    "        \"\"\"\n",
    "        featuresOdds = []\n",
    "\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        for feat in self.features:\n",
    "           # Formula for odds ratio: P(feature=1 | label1) / P(feature=1 | label2)\n",
    "            odds_ratio = (\n",
    "                self.conditionalProb[(feat, label1)] / self.conditionalProb[(feat, label2)]\n",
    "            )\n",
    "            featuresOdds.append((odds_ratio, feat))\n",
    "\n",
    "        # Sort and take the top 100\n",
    "        featuresOdds.sort(reverse=True)\n",
    "        featuresOdds = [feat for _, feat in featuresOdds[:100]]    \n",
    "        #util.raiseNotDefined()\n",
    "\n",
    "        return featuresOdds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf5fb9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tnaiveBayes\n",
      "training set size:\t700\n",
      "using automatic tuning for naivebayes\n",
      "Extracting features...\n",
      "Training...\n",
      "Performance validation set for k=0.001: 81.0%\n",
      "Performance validation set for k=0.010: 80.3%\n",
      "Performance validation set for k=0.050: 80.0%\n",
      "Performance validation set for k=0.100: 79.7%\n",
      "Performance validation set for k=0.500: 79.0%\n",
      "Performance validation set for k=1.000: 79.0%\n",
      "Performance validation set for k=5.000: 77.0%\n",
      "Performance validation set for k=10.000: 73.3%\n",
      "Performance validation set for k=20.000: 65.3%\n",
      "Performance validation set for k=50.000: 53.7%\n",
      "Validating...\n",
      "243 correct out of 300 (81.0%).\n",
      "Testing...\n",
      "211 correct out of 300 (70.3%).\n",
      "===================================\n",
      "Mistake on example 2\n",
      "Predicted 8; truth is 2\n",
      "Image: \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "           +++#####+        \n",
      "          +#########+       \n",
      "          +##########+      \n",
      "           ++++  +###+      \n",
      "                  +##+      \n",
      "                 +###+      \n",
      "                +###+       \n",
      "               +###+        \n",
      "              +####+        \n",
      "              +###+         \n",
      "             +###+          \n",
      "            +###+           \n",
      "           +###+            \n",
      "          +###+             \n",
      "         +####+             \n",
      "        +####+  ++++        \n",
      "       +#####++###++        \n",
      "      +##########+          \n",
      "       +#######+            \n",
      "         ++++               \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tnaiveBayes\n",
      "training set size:\t700\n",
      "using automatic tuning for naivebayes\n",
      "Extracting features...\n",
      "Training...\n",
      "Performance validation set for k=0.001: 81.0%\n",
      "Performance validation set for k=0.010: 80.3%\n",
      "Performance validation set for k=0.050: 80.0%\n",
      "Performance validation set for k=0.100: 79.7%\n",
      "Performance validation set for k=0.500: 79.0%\n",
      "Performance validation set for k=1.000: 79.0%\n",
      "Performance validation set for k=5.000: 77.0%\n",
      "Performance validation set for k=10.000: 73.3%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m args, options \u001b[38;5;241m=\u001b[39m readCommand( sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m:] ) \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Run classifier\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mrunClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 189\u001b[0m, in \u001b[0;36mrunClassifier\u001b[1;34m(args, options)\u001b[0m\n\u001b[0;32m    187\u001b[0m args, options \u001b[38;5;241m=\u001b[39m readCommand( sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m:] ) \n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Run classifier\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[43mrunClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 175\u001b[0m, in \u001b[0;36mrunClassifier\u001b[1;34m(args, options)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Conduct training and testing\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingLabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidationData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidationLabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    177\u001b[0m guesses \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mclassify(validationData)\n",
      "File \u001b[1;32mM:\\ML_Assignment\\Gabriel.Rosario.AIGS1003A1-main\\naiveBayes.py:37\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.train\u001b[1;34m(self, trainingData, trainingLabels, validationData, validationLabels)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     kgrid \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk]\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainAndTune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingLabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidationData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidationLabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkgrid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mM:\\ML_Assignment\\Gabriel.Rosario.AIGS1003A1-main\\naiveBayes.py:101\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.trainAndTune\u001b[1;34m(self, trainingData, trainingLabels, validationData, validationLabels, kgrid)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditionalProb \u001b[38;5;241m=\u001b[39m conditional_prob\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Evaluate performance \u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidationData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(predictions[i] \u001b[38;5;241m==\u001b[39m validationLabels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(validationLabels))) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(validationLabels)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformance validation set for k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mM:\\ML_Assignment\\Gabriel.Rosario.AIGS1003A1-main\\naiveBayes.py:122\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.classify\u001b[1;34m(self, testData)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposteriors \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Log posteriors are stored for later data analysis (autograder).\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datum \u001b[38;5;129;01min\u001b[39;00m testData:\n\u001b[1;32m--> 122\u001b[0m   posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculateLogJointProbabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m   guesses\u001b[38;5;241m.\u001b[39mappend(posterior\u001b[38;5;241m.\u001b[39margMax())\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposteriors\u001b[38;5;241m.\u001b[39mappend(posterior)\n",
      "File \u001b[1;32mM:\\ML_Assignment\\Gabriel.Rosario.AIGS1003A1-main\\naiveBayes.py:142\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.calculateLogJointProbabilities\u001b[1;34m(self, datum)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feat, value \u001b[38;5;129;01min\u001b[39;00m datum\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    141\u001b[0m         conditional_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditionalProb[(feat, label)]\n\u001b[1;32m--> 142\u001b[0m         logJoint[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(conditional_prob) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m value) \u001b[38;5;241m*\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconditional_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m#util.raiseNotDefined()\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logJoint\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dataClassifier\n",
    "\n",
    "#  parser.add_option('-c', '--classifier', help=default('The type of classifier'), choices=['mostFrequent', 'nb', 'naiveBayes', 'perceptron', 'mira', 'minicontest'], default='mostFrequent')\n",
    "#  parser.add_option('-d', '--data', help=default('Dataset to use'), choices=['digits', 'faces'], default='digits')\n",
    "#  parser.add_option('-t', '--training', help=default('The size of the training set'), default=100, type=\"int\")\n",
    "#  parser.add_option('-a', '--autotune', help=default(\"Whether to automatically tune hyperparameters\"), default=False, action=\"store_true\")\n",
    "#  parser.add_option('-i', '--iterations', help=default(\"Maximum iterations to run training\"), default=3, type=\"int\")\n",
    "\n",
    "\n",
    "#sys.argv = ['dataClassifier.py', '-h'] \n",
    "#sys.argv = ['dataClassifier.py', '-c', 'naiveBayes', '--autotune'] \n",
    "#sys.argv = ['dataClassifier.py', '-a', '-d', 'digits', '-c', 'naiveBayes', '--autotune', '--training', '700'] \n",
    "\n",
    "#The type of classifier = choices=['mostFrequent', 'nb', 'naiveBayes', 'perceptron', 'mira', 'minicontest']\n",
    "\n",
    "#Dataset to use = ['digits', 'faces']\n",
    "\n",
    "#Maximum iterations to run training  --iterations = 3\n",
    "\n",
    "sys.argv = ['dataClassifier.py', '-c', 'naiveBayes', '--autotune', '-t', '700', '-i', '4', '-d', 'digits'] \n",
    "\n",
    "\n",
    "\n",
    "#print(user_args)\n",
    "# Read input\n",
    "args, options = readCommand( sys.argv[1:] ) \n",
    "# Run classifier\n",
    "runClassifier(args, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb05443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
